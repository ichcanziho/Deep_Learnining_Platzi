# Curso de Desarrollo de Chatbots con OpenAI

Aprende a usar la API de OpenAI para desarrollar un chatbot con identidad propia. Da este paso hacia una experiencia conversacional impulsada por la inteligencia artificial con GPT-4, GPT-3.5 y Davinci.

- Integra un LLM de OpenAI a una aplicaci√≥n de chat.
- Estima el costo de consumo por tokens de la API.
- Aplica fine-tuning a un modelo para personalizarlo.


> ## NOTA:
> Antes de continuar te invito a que revises los cursos anteriores:
> - [1: Curso profesional de Redes Neuronales con TensorFlow](https://github.com/ichcanziho/Deep_Learnining_Platzi/tree/master/1%20Curso%20de%20fundamentos%20de%20redes%20neuronales)
> - [2: Curso de Redes Neuronales Convolucionales con Python y keras](https://github.com/ichcanziho/Deep_Learnining_Platzi/tree/master/2%20Curso%20de%20Redes%20Neuronales%20Convolucionales)
> - [3: Curso profesional de Redes Neuronales con TensorFlow](https://github.com/ichcanziho/Deep_Learnining_Platzi/tree/master/3%20Curso%20profesional%20de%20Redes%20Neuronales%20con%20TensorFlow)
> - [4: Curso de Transfer Learning con Hugging Face](https://github.com/ichcanziho/Deep_Learnining_Platzi/tree/master/4%20Curso%20de%20Transfer%20Learning%20con%20Hugging%20Face)
> - [5: Curso de Experimentaci√≥n en Machine Learning con Hugging Face](https://github.com/ichcanziho/Deep_Learnining_Platzi/tree/master/5%20Curso%20de%20introducci%C3%B3n%20a%20Demos%20de%20Machine%20Learning%20con%20Hugging%20Face)
> - [6: Curso de detecci√≥n y segmentaci√≥n de objetos con TensorFlow](https://github.com/ichcanziho/Deep_Learnining_Platzi/tree/master/6%20Curso%20de%20detecci%C3%B3n%20y%20segmentaci%C3%B3n%20de%20objetos%20con%20Tensorflow)
> - [7: Curso profesional de Computer Vision con TensorFlow](https://github.com/ichcanziho/Deep_Learnining_Platzi/tree/master/7%20Curso%20profesional%20de%20Computer%20Vision%20con%20TensorFlow)
> - [8: Curso de generaci√≥n de im√°genes](https://github.com/ichcanziho/Deep_Learnining_Platzi/tree/master/8%20Curso%20de%20generaci%C3%B3n%20de%20im%C3%A1genes)
> - [9: Cursos de Fundamentos de NLP](https://github.com/ichcanziho/Deep_Learnining_Platzi/tree/master/9%20Curso%20de%20Fundamentos%20de%20NLP)
> - [10: Curso de Fundamentos de Procesamiento de Lenguaje Natural con Python y NLTK](https://github.com/ichcanziho/Deep_Learnining_Platzi/tree/master/10%20Curso%20de%20Algoritmos%20de%20Clasificaci%C3%B3n%20de%20Texto)
> - [11: Curso de redes Neuronales con PyTorch](https://github.com/ichcanziho/Deep_Learnining_Platzi/tree/master/11%20Curso%20de%20Redes%20Neuronales%20con%20PyTorch%20)
> 
> Este Curso es el N√∫mero 12 de una ruta de Deep Learning, quiz√° algunos conceptos no vuelvan a ser definidos en este repositorio,
> por eso es indispensable que antes de empezar a leer esta gu√≠a hayas comprendido los temas vistos anteriormente.
> 
> Sin m√°s por agregar disfruta de este curso


# √çNDICE:

- [1 OpenAI API](#1-openai-api)
  - [1.1 ¬øC√≥mo usar la API de OpenAI en tu producto?](#11-c√≥mo-usar-la-api-de-openai-en-tu-producto)
  - [1.2 Conociendo la documentaci√≥n de la API de OpenAI](#12-conociendo-la-documentaci√≥n-de-la-api-de-openai)
  - [1.3 Cargar modelo de la API de OpenAI con Python](#13-cargar-modelo-de-la-api-de-openai-con-python)
  - [1.4 Creaci√≥n de ejemplo utilizando la API de OpenAI](#14-creaci√≥n-de-ejemplo-utilizando-la-api-de-openai)
  - [1.5 Par√°metros de Text Completion: temperature, top_p y n](#15-par√°metros-de-text-completion-temperature-topp-y-n)
  - [1.6 Buenas pr√°cticas al usar modelos de OpenAI](#16-buenas-pr√°cticas-al-usar-modelos-de-openai)
  - [1.7 Chat Completions](#17-chat-completions)
  - [1.8 Actualizaciones de la API de OpenAI: GPT-4 disponible y modelos deprecados](#18-actualizaciones-de-la-api-de-openai-gpt-4-disponible-y-modelos-deprecados)
  - [Quiz de OpenAI API](#quiz-de-openai-api)
- [2 Fine-tuning de modelos de OpenAI](#2-fine-tuning-de-modelos-de-openai)
  - [2.1 ¬øPor qu√© hacer fine-tuning a modelos de OpenAI?](#21-por-qu√©-hacer-fine-tuning-a-modelos-de-openai)
  - [2.2 Costos de uso de OpenAI: tokenizaci√≥n de texto](#22-costos-de-uso-de-openai-tokenizaci√≥n-de-texto)
  - [2.3 Configuraci√≥n de entorno local de OpenAI con Anaconda](#23-configuraci√≥n-de-entorno-local-de-openai-con-anaconda)
  - [2.4 Formato de datos para fine-tuning](#24-formato-de-datos-para-fine-tuning)
  - [2.5 Preparar datos para fine-tuning](#25-preparar-datos-para-fine-tuning)
  - [2.6 Fine-tuning de modelo de OpenAI](#26-fine-tuning-de-modelo-de-openai)
  - [2.7 ¬øC√≥mo usar PlayGround de OpenAI para probar modelos?](#27-c√≥mo-usar-playground-de-openai-para-probar-modelos)
  - [2.8 Pruebas al modelo con fine-tuning](#28-pruebas-al-modelo-con-fine-tuning)
  - [2.9 Optimizar el modelo: ajuste de par√°metros en Playground](#29-optimizar-el-modelo-ajuste-de-par√°metros-en-playground)
  - [2.10 Validaci√≥n de modelos fine-tuned de OpenAI](#210-validaci√≥n-de-modelos-fine-tuned-de-openai)
  - [Quiz de fine-tuning de modelos de OpenAI](#quiz-de-fine-tuning-de-modelos-de-openai)
- [3 Integraci√≥n de modelo a aplicaci√≥n de chat](#3-integraci√≥n-de-modelo-a-aplicaci√≥n-de-chat)
  - [3.1 ¬øC√≥mo crear un chatbot con Telegram?](#31-c√≥mo-crear-un-chatbot-con-telegram)
  - [3.2 Procesando la entrada del usuario para el chatbot](#32-procesando-la-entrada-del-usuario-para-el-chatbot)
  - [3.3 Prueba de env√≠o de mensajes del chatbot](#33-prueba-de-env√≠o-de-mensajes-del-chatbot)
  - [3.4 Funci√≥n main() del chatbot](#34-funci√≥n-main-del-chatbot)
  - [3.5 Integraci√≥n del modelo de OpenAI a Telegram](#35-integraci√≥n-del-modelo-de-openai-a-telegram)
  - [3.6 Manejo de errores y excepciones de la API de OpenAI](#36-manejo-de-errores-y-excepciones-de-la-api-de-openai)
  -[Quiz de integraci√≥n de LLM a chat](#quiz-de-integraci√≥n-de-llm-a-chat)
- [4 Conclusi√≥n](#4-conclusi√≥n)
  - [4.1 Recomendaciones finales y proyectos alternativos con el API de OpenAI](#41-recomendaciones-finales-y-proyectos-alternativos-con-el-api-de-openai)

# 1 OpenAI API

## 1.1 ¬øC√≥mo usar la API de OpenAI en tu producto?

Entre las tareas m√°s comunes que podemos hacer utilizando LLM's tenemos:

![1.png](ims%2F1%2F1.png)

- **Clasificaci√≥n**: Los LLM's se pueden utilizar para clasificar textos en categor√≠as espec√≠ficas. Por ejemplo, pueden ser entrenados para clasificar correos electr√≥nicos como "spam" o "no spam", noticias como "pol√≠tica" o "deportes", comentarios como "positivos" o "negativos", entre otros.

- **Generaci√≥n**: Los LLM's son muy √∫tiles para generar texto coherente y cohesivo a partir de un prompt o una pregunta inicial. Pueden ser utilizados para generar respuestas autom√°ticas, redacciones, res√∫menes de texto, descripciones de im√°genes, entre otros.

- **Traducci√≥n**: Los LLM's tambi√©n pueden ser utilizados para tareas de traducci√≥n autom√°tica. Al entrenarlos con datos de pares de idiomas, se puede lograr que el modelo genere traducciones coherentes y precisas de textos en diferentes idiomas.

- **Chatbots**: Los LLM's son una herramienta fundamental para la creaci√≥n de chatbots. Pueden ser entrenados con datos de di√°logos para que el modelo pueda responder preguntas, entablar conversaciones y simular interacciones humanas de manera natural.

- **Programaci√≥n**: Los LLM's tambi√©n pueden utilizarse para generar c√≥digo o ayudar en tareas de programaci√≥n. Pueden ser entrenados con datos de c√≥digo fuente y utilizados para autocompletar c√≥digo, proporcionar sugerencias de c√≥digo, corregir errores o incluso generar c√≥digo completo a partir de una descripci√≥n.

En este curso vamos a utilizar modelos tipo GPT para tareas relacionadas con Texto.

Veamos nuestro primer ejemplo de C√≥digo. ¬øC√≥mo acceder a API de OpenAI desde c√≥digo en Python?

> ## Nota: 
> El c√≥digo de esta secci√≥n lo puedes encontrar en: [1_tweet_sentiment.py](scripts%2F1_tweet_sentiment.py)

> Antes de continuar debemos instalar un par de librer√≠as en python

En este ejercicio vamos a utilizar Chat GPT para hacer un clasificador de Sentimientos **positivos, neutrales, negativos** de **tweets**,

```bash
pip install python-dotenv
pip install openai
```

Antes de continuar asegurate de generar tu primer **API KEY** de OpenAI: https://platform.openai.com/account/api-keys

![2.png](ims%2F1%2F2.png)

Esta API KEY ES PERSONAL cu√≠dala mucho y NO la compartas con nadie. Una vez generada tienes que guardarla porque no podr√°s volver
a acceder a ella desde la p√°gina de OpenAI.

Personalmente, voy a crear un archivo llamado `ap.env` en donde almacenar√© mi `API KEY` de OpenAI

```commandline
OPENAI_API_KEY=sk-clavellenadeamor
```
De esta manera en Python cuando quiera acceder a ella no tendr√© que tenerla visible al p√∫blico. Vamos a empezar importando
las bibliotecas y accediendo a nuestra variable de entorno.
```python
import os
from dotenv import load_dotenv
import openai

# Carga las variables de entorno desde el archivo .env
load_dotenv("../envs/ap.env")
openai.api_key = os.getenv("OPENAI_API_KEY")
```

Ahora vamos a conocer un poco sobre nuestro primer m√©todo: `openai.Completion.create()`

- **model**: Especifica el modelo de lenguaje que se utilizar√° para generar el texto. Puedes elegir entre diferentes modelos, como "gpt-3.5-turbo" o "text-davinci-003". Cada modelo tiene diferentes caracter√≠sticas y capacidades, por lo que es importante seleccionar el adecuado para tu caso de uso.

- **prompt**: Es el texto inicial o la consulta que proporcionas al modelo como punto de partida para generar el texto continuado. Puedes utilizar un prompt espec√≠fico para guiar al modelo en la direcci√≥n deseada o para establecer el contexto para la generaci√≥n de texto.

- **temperature**: Controla la aleatoriedad de las respuestas generadas por el modelo. Un valor m√°s bajo, como 0.2, produce respuestas m√°s determin√≠sticas y coherentes, mientras que un valor m√°s alto, como 0.8, genera respuestas m√°s creativas pero potencialmente menos coherentes.

- **max_tokens**: Define el n√∫mero m√°ximo de tokens que se generar√°n en la respuesta. Un token puede ser una palabra o un car√°cter, seg√∫n el modelo que est√©s utilizando. Si deseas limitar la longitud del texto de salida, puedes establecer este par√°metro en un valor adecuado.

- **top_p**: Tambi√©n conocido como "nucleus sampling" o "restricci√≥n de token de parche". Limita la generaci√≥n de texto a una distribuci√≥n de probabilidad acumulativa superior a un cierto umbral. Un valor m√°s bajo, como 0.2, har√° que las respuestas sean m√°s restrictivas y enfocadas, mientras que un valor m√°s alto, como 0.8, permitir√° una mayor diversidad en las respuestas generadas.

- **frequency_penalty**: Este par√°metro controla la preferencia del modelo por evitar la repetici√≥n de frases. Un valor m√°s alto, como 0.6, penalizar√° m√°s la repetici√≥n y har√° que el modelo evite generar frases similares. Un valor de 0 no penaliza la repetici√≥n.

- **presence_penalty**: Este par√°metro controla la preferencia del modelo por evitar la menci√≥n de ciertas palabras o temas en el texto de salida. Un valor m√°s alto, como 0.6, penalizar√° m√°s la presencia de ciertas palabras o temas y har√° que el modelo evite mencionarlos. Un valor de 0 no penaliza la presencia de palabras o temas espec√≠ficos.


Ahora vamos a presentar nuestra primera petici√≥n al API de OpenAI:

```python
response = openai.Completion.create(
  model="text-davinci-003",
  prompt="Decide si el sentimiento de un Tweet es positivo, neutral, o negativo. \
  \n\nTweet: \"#LoNuevoEnPlatzi es el Platzibot ü§ñ. Un asistente creado con Inteligencia Artificial para acompa√±arte en tu proceso de aprendizaje.\
  \"\nSentiment:",
  temperature=0,
  max_tokens=60,
  top_p=1.0,
  frequency_penalty=0.5,
  presence_penalty=0.0
)
print(response.choices[0].text)
```
Respuesta esperada:
```commandline
 Positivo
```

Excelente, en esta clase hemos aprendido a conectarnos al API de OpenAI a trav√©s de su biblioteca en Python, hemos creado
un archivo .env para almacenar nuestra API KEY y hemos conocido algunos de los par√°metros b√°sicos que tiene el m√©todo
`Completion.create` y hemos logrado hacer un clasificador de sentimientos de tweets.

## 1.2 Conociendo la documentaci√≥n de la API de OpenAI

Es indispensable que tengamos muy presente en todo momento la Documentaci√≥n de OpenAI: https://platform.openai.com/docs/introduction/overview

![3.png](ims%2F1%2F3.png)

Este sitio ser√° nuestro mejor aliado para conocer a profundidad las caracter√≠sticas de los modelos y servicios que nos puede
ofrecer OpenAI. 

Algunos de los enlaces m√°s interesantes a los que debemos prestar atenci√≥n son:

- [Models](https://platform.openai.com/docs/models)
  ![4.png](ims%2F1%2F4.png)
- [Fine-tuning](https://platform.openai.com/docs/guides/fine-tuning)
  ![5.png](ims%2F1%2F5.png)
- [Chat Completions](https://platform.openai.com/docs/guides/gpt/chat-completions-api)
  ![6.png](ims%2F1%2F6.png)

Cada uno de estos elementos los estaremos visitando a lo largo del curso, sin embargo, es importante tener en cuenta que hay
mucho m√°s que explorar en la documentaci√≥n y que la misma tiene tutoriales que nos pueden ense√±ar a como utilizarla.

Finalmente, tambi√©n podemos leer su [API REFERENCE](https://platform.openai.com/docs/api-reference)

![7.png](ims%2F1%2F7.png)

Es similar a la documentaci√≥n, pero est√° m√°s orientada en como consumir sus servicios desde el API y sus tutoriales son a menor profundidad.

## 1.3 Cargar modelo de la API de OpenAI con Python

Esta clase ser√° un repaso de la primer clase en donde veremos como f√°cilmente cargar nuestra API KEY, seleccionar un modelo y finlamente
enviar una prompt a chat GPT desde c√≥digo:

> ## Nota:
> Puedes encontrar el c√≥digo en: [2_cargar_modelo.py](scripts%2F2_cargar_modelo.py)

```python
import os
from dotenv import load_dotenv
import openai

load_dotenv("../envs/ap.env")
openai.api_key = os.getenv("OPENAI_API_KEY")

response = openai.Completion.create(
    model="text-davinci-003",
    prompt="¬øQui√©n descubri√≥ Am√©rica?",
    max_tokens=100
)

print(response.choices[0].text)
```

Respuesta esperada:
```commandline
Ninguna persona descubri√≥ Am√©rica, ya que el continente ya estaba habitado por pueblos ind√≠genas cuando los europeos llegaron. El navegante generalmente atribuido como el descubridor de Am√©rica es Crist√≥bal Col√≥n, quien realiz√≥ su primera expedici√≥n en 1492.
```
Aclaraci√≥n, es probable que tu respuesta sea diferente a la que yo he conseguido. ChatGPT no necesariamente va a responder exactamente igual a la misma pregunta.

## 1.4 Creaci√≥n de ejemplo utilizando la API de OpenAI

En este ejercicio vamos a crear un peque√±o juego de `adivina el animal`. Primero dado una lista de animales seleccionaremos
uno de ellos al azar y vamos a dar una pista gen√©rica al usuario. Su trabajo es intentar adivinar el animal. En cada itento
cuando el usuario falle, vamos a hacer que ChatGPT gener√© una nueva pista que mencione atributos del animal en cuesti√≥n, evitando
mencionar el nombre del animal, esto ser√° indefinido hasta que el usuario adivine el animal.

> ## Nota:
> El c√≥digo lo puedes encontrar en: [3_adivina_animal.py](scripts%2F3_adivina_animal.py)

Empezamos importando las bibliotecas necesarias:

```python
from dotenv import load_dotenv
import random
import openai
import os
```

Creamos nuestra funci√≥n que elige un animal al azar y brinda la primera pista:

```python
def get_base_clue():
    words = ['elefante', 'le√≥n', 'jirafa', 'hipop√≥tamo', 'mono']
    random_word = random.choice(words)
    prompt = 'Adivina la palabra que estoy pensando. Es un animal que vive en la selva.'
    return prompt, random_word
```

Ahora crearemos nuestro c√≥digo de generaci√≥n de nuevas pistas, que utiliza a ChatGPT para que dado un prompt me pueda regresar
caracter√≠sticas de cierto animal.

```python
def get_new_clue(animal):
    response = openai.Completion.create(
        engine='text-davinci-003',
        prompt='Dame una caracteristica del tipo animal' + animal + ', pero jam√°s digas el nombre del animal',
        max_tokens=100)
    return response.choices[0].text
```

A continuaci√≥n vamos a programar la mec√°nica principal del juego:

```python
def play_game():
    # Empezamos con nuestro animal aleatorio y primer pista gen√©rica
    prompt, real_animal = get_base_clue()
    print(prompt)
    # Mientras la respuesta del usuario sea diferente al verdadero animal
    while (user_input := input("Ingresa tu respuesta: ")) != real_animal:
        # Le decimos que se equivoc√≥
        print('Respuesta incorrecta. Intentalo de nuevo')
        # Y le damos una nueva pista
        print(get_new_clue(real_animal))
    # Si salimos del ciclo while es porque el usuario ha acertado
    print('Correcto! La respuesta era:', real_animal)
```
Y finalmente como ya tenemos todos los ingredientes, vamos a crear nuestro punto de acceso:
```python
if __name__ == '__main__':
    load_dotenv("../envs/ap.env")
    openai.api_key = os.getenv("OPENAI_API_KEY")
    play_game()
```
Respuesta esperada:
```commandline
Adivina la palabra que estoy pensando. Es un animal que vive en la selva.
Ingresa tu respuesta: gato
Respuesta incorrecta. Intentalo de nuevo


Cuerpo voluminoso con piel √°spera.
Ingresa tu respuesta: elefante
Respuesta incorrecta. Intentalo de nuevo


Tiene una piel gruesa y desagradable al tacto.
Ingresa tu respuesta: hipop√≥tamo
Correcto! La respuesta era: hipop√≥tamo
```
Excelente, ya hemos desarrollado un minijuego con ayuda del API de OpenAI.

## 1.5 Par√°metros de Text Completion: temperature, top_p y n

Pese a que esta clase s√≠ tiene c√≥digo, es b√°sicamente el mismo que ya hemos visto en: [2_cargar_modelo.py](scripts%2F2_cargar_modelo.py)
As√≠ que en realidad solo vamos a repasar una breve explicaci√≥n de los par√°metros que dispone `Completion`

Para obtener la informaci√≥n directamente desde su Api Reference https://platform.openai.com/docs/api-reference/completions/create

**Par√°metros de Text Completion**

- **model**: ID del modelo a utilizar.
- **prompt**: Las solicitudes para generar finalizaciones, codificadas como una cadena, una matriz de cadenas, una matriz de tokens o una matriz de matrices de tokens.
- **suffix**:Predeterminado a nulo
El sufijo que viene despu√©s de completar el texto insertado.
- **max_tokens**: Predeterminado a 16
El n√∫mero m√°ximo de tokens a generar en la finalizaci√≥n.
- **temperature**: Predeterminado a 1
Qu√© temperatura de muestreo usar, entre 0 y 2. Los valores m√°s altos, como 0,8, har√°n que la salida sea m√°s aleatoria, mientras que los valores m√°s bajos, como 0,2, la har√°n m√°s enfocada y determinista.
Usa esto o top_p pero no ambos.
- **top_p**: Predeterminado a 1
Una alternativa al muestreo con temperatura, llamado muestreo de n√∫cleo, donde el modelo considera los resultados de los tokens con masa de probabilidad top_p. Por lo tanto, 0.1 significa que solo se consideran las fichas que comprenden el 10 % de la masa de probabilidad superior.
- **n**: Predeterminado a 1
Cu√°ntas completions generar para cada prompt.
>Nota: debido a que este par√°metro genera muchas finalizaciones, puede consumir r√°pidamente su cuota de token. √öselo con cuidado y aseg√∫rese de tener configuraciones razonables para max_tokens y stop.
- **stream**: Predeterminado a falso
Ya sea para transmitir el progreso parcial. Si se establece, los tokens se enviar√°n como eventos enviados por el servidor solo de datos a medida que est√©n disponibles, y la secuencia terminar√° con un mensaje de data: [DONE]. [Ejemplo en python](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb)
- **logprobs**: Si logprobs es 5, la API devolver√° una lista de los 5 tokens m√°s probables. La API siempre devolver√° el logprob del token muestreado, por lo que puede haber hasta logprobs+1 elementos en la respuesta.
- **echo**: Repita el prompt adem√°s de la finalizaci√≥n(Predeterm: Falso)
stop: Hasta 4 secuencias donde la API dejar√° de generar m√°s tokens. El texto devuelto no contendr√° la secuencia de parada.
- **best_of**: Predeterminado a 1
Genera el mejor de completions del lado del servidor y devuelve el ‚Äúmejor‚Äù (el que tiene la mayor probabilidad de registro por token). Los resultados no se pueden transmitir(stream).
- **user**: Un identificador √∫nico que representa a su usuario final, que puede ayudar a OpenAI a monitorear y detectar abusos. Aprende m√°s.

## 1.6 Buenas pr√°cticas al usar modelos de OpenAI

Es necesario tener en cuenta el costo del servicio del API de OpenAI

https://platform.openai.com/account/usage

![8.png](ims%2F1%2F8.png)
Aqu√≠ podemos ver el consumo de dinero que hemos tenido por cada d√≠a. Pero tambi√©n estamos limitados no solamente en t√©rminos 
de efectivo, sino tambi√©n en t√©rminos de: `Tokens Per Minute TPM` y `Request Per Minute` de acuerdo al modelo y acci√≥n que estemos haciendo:

https://platform.openai.com/account/rate-limits
![9.png](ims%2F1%2F9.png)

Diferentes miembros tienen diferentes roles, y solamente los miembros `owner` tienen acceso al `billing` o facturaci√≥n.
Lo m√°s importante a destacar es que existen dos conceptos:

> Nota, la siguiente imagen es tomada directo de la clase:

![10.png](ims%2F1%2F10.png)

Podemos definir un `soft limit` que es una Advertencia, OpenAI nos avisar√° por correo que ya hemos alcanzado este l√≠mite pero
no nos va a poner restricciones para seguir utiliz√°ndolo.

Mientras que si llegamos al `hard limit` entonces ah√≠ se detendr√°n operaciones. 
Tambi√©n podemos pedir un `Request Increase` para aumentar nuestro l√≠mite mensual.

**Buenas pr√°cticas al usar modelos de OpenAI**

- Uso:
  - Especifica claramente tu solicitud. Mayor contexto.
  - Utiliza la instrucci√≥n inicial
  - Controla la longitud de la respuesta.
  - Experimenta con la temperatura
- Facturaci√≥n (Billing):
  - Consumo de usuarios y facturaci√≥n.
  - Soft Limit y hard Limit:
  - Pedir si queremos aumentar l√≠mites.
  - Precios de Fine tuning models: https://openai.com/pricing
- Seguridad:
  - Gesti√≥n y soluci√≥n de problemas.
  - √âtica y consideraciones legales.
  - Privacidad de los datos.
  - Control de users(Owner y Reader).

## 1.7 Chat Completions

En esta clase vamos a ver ejemplos simples de como utilizar [Chat Completions API](https://platform.openai.com/docs/guides/gpt/chat-completions-api)
Es similar al m√©todo pasado, pero permite brindar mayor contexto al modelo.

Tambi√©n puedes ver m√°s informaci√≥n en [Create Chat Completion](https://platform.openai.com/docs/api-reference/chat/create)

> ## Nota:
> Puedes ver el c√≥digo de en: [4_chat_completion.py](scripts%2F4_chat_completion.py)

Partimos de la misma base que hemos estado utilizando en los otros ejemplos:

```python
import os
from dotenv import load_dotenv
import openai

load_dotenv("../envs/ap.env")
openai.api_key = os.getenv("OPENAI_API_KEY")
```
Sin embargo, aqu√≠ viene el primer cambio. Ahora vamos a utilizar `openai.ChatCompletion.create`
y veremos que la estructura es ligeramente diferente, en lugar de enviar simplemente un `prompt` sencillo vamos a enviar
una lista de diccionarios en `messages`:

```python
response = openai.ChatCompletion.create(
    model='gpt-3.5-turbo',
    messages=[
        {"role": "system", "content": "Eres un asistente que da informacion sobre deportes"},
        {"role": "user", "content": "¬øQui√©n gan√≥ el mundial de f√∫tbol?"},
        {"role": "assistant", "content": "El mundial de 2022 lo gan√≥ Argentina"},
        {"role": "user", "content": "¬øD√≥nde se jug√≥?"}
    ],
    temperature=1,  # Este es el valor default
    max_tokens=60

)
print(response['choices'][0]['message']['content'])
print("*"*64)
```
Respuesta esperada:
```commandline
El Mundial de f√∫tbol de 2022 se jug√≥ en Qatar.
```

Podemos observar como para utilizar `ChatCompletion` debemos empezar dandole al modelo un `contexto` general al modelo. 

- Eres un asistente ...

Despu√©s, podemos darle un ejemplo de la posible entrada que tendr√≠a un usuario:

- ¬øQui√©n gano el mundial de futbol?

Y finalmente, podemos darle un ejemplo de la respuesta que va a dar el modelo siendo √©l el `assistant`:

- El mundial de 2022 lo gan√≥ Argentina.

Adicionalmente, podemos leer la documentaci√≥n del API [Create Chat Completion](https://platform.openai.com/docs/api-reference/chat/create)
para leer los dem√°s par√°metros del modelo. Aunque muchos son los mismos que ya hemos visto anteriormente.
Como resumen presentamos la siguiente diapositiva.

![12.png](ims%2F1%2F12.png)

Y finalmente, si te preguntas cuando debo usar Completion vs ChatCompletion
![11.png](ims%2F1%2F11.png)

Podemos ver que en general Completion sirve con todos los modelos, pero solamente est√° esperando generar
una sola `completion` no tener un di√°logo profundo o conversaci√≥n de multiples turno.

Adicionalmente, utilizar `ChatCompletion` nos permite dar un formato de Contexto, Entrada, Respuesta esperada, lo cu√°l es 
muy √∫til para un sin fin de aplicaciones. 


## 1.8 Actualizaciones de la API de OpenAI: GPT-4 disponible y modelos deprecados

OpenAI y su API es un producto que est√° en constante actualizaci√≥n por la r√°pida innovaci√≥n que actualmente tienen las tecnolog√≠as de IA.

‚ö†Ô∏èRecuerda estar al pendiente de este curso porque seguir√° actualiz√°ndose de acuerdo a los cambios en OpenAI. Cada vez que haya una actualizaci√≥n importante podr√°s enterarte en tu correo electr√≥nico y en un agregado en esta clase de lectura.

### Disponibilidad general de la API de GPT-4

GPT-4 es el modelo m√°s capaz de OpenAI. En la √∫ltima actualizaci√≥n de la API, todo mundo puede acceder libremente a la API de GPT-4 con un contexto de 8K. Ya no es necesario estar dentro de alguna beta.

Se planea que este modelo de Chat est√© disponible en todas sus versiones completando el mes de julio de 2023 seg√∫n la disponibilidad de recursos inform√°ticos. Pero es muy posible que ya puedas utilizarlo en el modo ChatCompletion.

üì£ Se est√° trabajando en habilitar de manera segura fine-tuning para GPT-4 y GPT-3.5 Turbo. Se espera que esta funci√≥n est√© disponible m√°s adelante este a√±o.

Fuente: [GPT-4 API general availability and deprecation of older models in the Completions API (openai.com)](https://openai.com/blog/gpt-4-api-general-availability)

### Pasando de text completions a chat completions (modelos deprecados)

La API de Completions fue introducida en junio de 2020 para proporcionar una solicitud de texto de forma libre para interactuar con nuestros modelos de lenguaje. Desde entonces, se ha aprendido que podemos obtener mejores resultados con una interfaz de solicitud m√°s estructurada.

El paradigma basado en chat ha demostrado ser poderoso, manejando la gran mayor√≠a de los casos de uso anteriores y las nuevas necesidades de conversaci√≥n, al tiempo que brinda una mayor flexibilidad y especificidad. En particular, la interfaz estructurada de la API de Chat Completions y las capacidades de conversaci√≥n de m√∫ltiples turnos permiten crear experiencias conversacionales y una amplia gama de tareas de completado.

OpenAI planea seguir invirtiendo la mayor parte de esfuerzos en Chat Completions, ya que ofrecer√° una experiencia cada vez m√°s capaz y f√°cil de usar. Es por ello que para enero de 2024 retirar√°n algunos modelos anteriores de Text Completions. **Si bien esta API seguir√° siendo accesible, a partir de esta actualizaci√≥n est√° etiquetada como ‚Äúlegacy‚Äù en la plataforma.** No hay planes de lanzar nuevos modelos utilizando la API de Text Completions.

**A partir del 4 de enero de 2024, los modelos antiguos de text completions ya no estar√°n disponibles y ser√°n reemplazados por los siguientes modelos:**

![13.png](ims%2F1%2F13.png)

‚ö†Ô∏èEstos nuevos modelos estar√°n disponibles en las pr√≥ximas semanas para pruebas tempranas al especificar los siguientes nombres de modelos en las llamadas a la API:

- ada-002
- babbage-002
- curie-002
- davinci-002
- gpt-3.5-turbo-instruct

- Te sugerimos usar estos nuevos modelos para aplicarles fine-tuning en las siguientes clases de este curso, si es que ya los tienes disponibles en tu cuenta de OpenAI.

Fuente: [GPT-4 API general availability and deprecation of older models in the Completions API (openai.com)](https://openai.com/blog/gpt-4-api-general-availability)


## Quiz de OpenAI API

![14.png](ims%2F1%2F14.png)

![15.png](ims%2F1%2F15.png)

![16.png](ims%2F1%2F16.png)

![17.png](ims%2F1%2F17.png)

![18.png](ims%2F1%2F18.png)


# 2 Fine-tuning de modelos de OpenAI

## 2.1 ¬øPor qu√© hacer fine-tuning a modelos de OpenAI?

Cuando hablamos de fine-tunining nos referimos a refinamiento de un modelo pre-entrenado para poder cumplir con tareas 
m√°s espec√≠ficas para las que originalmente no fue necesariamente entrenado.

![1.png](ims%2F2%2F1.png)

Algunos de los beneficios que podemos mencionar sobre hacer fine-tuning a modelos de DL son:

**1. Adaptaci√≥n a tareas espec√≠ficas:** El fine-tuning permite ajustar el modelo preentrenado para tareas espec√≠ficas o dominios particulares. Esto es especialmente √∫til cuando se necesita que el modelo realice una tarea espec√≠fica, como la generaci√≥n de respuestas en un servicio de atenci√≥n al cliente, la traducci√≥n de texto en un idioma particular o la redacci√≥n de contenido especializado.

**2. Mejora del rendimiento:** Al ajustar el modelo a una tarea espec√≠fica, el fine-tuning puede mejorar significativamente su rendimiento y precisi√≥n en esa tarea en comparaci√≥n con usar el modelo preentrenado directamente.

**3. Reducci√≥n del tiempo de entrenamiento:** El preentrenamiento inicial de modelos de lenguaje como ChatGPT es una tarea intensiva en recursos y tiempo. Sin embargo, una vez que el modelo est√° preentrenado, el fine-tuning es un proceso m√°s r√°pido y eficiente en comparaci√≥n con el entrenamiento completo desde cero.

**4. Control y personalizaci√≥n:** Fine-tuning permite a los desarrolladores tener un mayor control sobre el modelo, lo que les permite ajustar y personalizar su comportamiento seg√∫n las necesidades espec√≠ficas de su aplicaci√≥n.

**5. Adquisici√≥n de conocimiento espec√≠fico:** Al entrenar el modelo en datos espec√≠ficos de una tarea, el modelo puede adquirir conocimiento relevante y especializado para esa tarea, lo que lo hace m√°s eficaz en su desempe√±o.

**6. Adaptaci√≥n a cambios en datos o requisitos:** Si los datos o las necesidades de una tarea cambian con el tiempo, el modelo puede ser sometido a un nuevo proceso de fine-tuning para adaptarse a esos cambios sin tener que volver a preentrenar desde cero.

Algunos problemas m√°s populares por los cuales decidimos hacer fine-tuning son:

- **Clasificaci√≥n**:
  - ¬øEl Modelo est√° haciendo declaraciones falsas?
  - An√°lisis de sentimientos
  - Categorizaci√≥n de correo electr√≥nico(clasificaci√≥n de spam).
- **Generaci√≥n condicional**(Crear conocimiento a partir de otro ya creado):
  - Ejemplo: tomar un texto de wiki y crear uno nuevo a partir de este.
  - Extracci√≥n de entidades(ver contexto del texto).
  - Chatbot de atenci√≥n al cliente.
  - Descripci√≥n basada en una lista t√©cnica de propiedades.(Dar descripciones de productos basadas en requermientos)


## 2.2 Costos de uso de OpenAI: tokenizaci√≥n de texto

Uno de los procesos principales que utiliza el API de OpenAI para procesar el texto, es la tokenizaci√≥n del mismo. En el contexto de ChatGPT, los tokens son las unidades m√°s peque√±as en las que el texto se divide para que el modelo pueda procesarlo. Un token puede ser tan corto como un car√°cter o tan largo como una palabra completa, y el proceso de dividir el texto en estos tokens se conoce como tokenizaci√≥n.
Sin embargo, la relaci√≥n Palabra a Token NO es exactamente 1:1 de hecho, una regla de dedo es que aproximadamente 100 tokens representan 75 palabras en el idioma ingl√©s, o 
tambi√©n podemos verlo como que la cantidad de palabras multiplicada por 1.33 es aproximadamente igual al n√∫mero de tokens que se van a utilizar.

Podemos utilizar [Tokenizer](https://platform.openai.com/tokenizer) para darnos una idea de como OpenAI procesa la cantidad de tokens por oraci√≥n.

![2.png](ims%2F2%2F2.png)

Esta informaci√≥n es indispensable tenerla en cuenta, porque cada modelo tiene un precio diferente tanto por procesar la informaci√≥n de entrada
como por generar la respuesta de salida.

![3.png](ims%2F2%2F3.png)

Esta informaci√≥n es bastante interesante cuando vamos a la secci√≥n de **Fine-tuning models:**

![4.png](ims%2F2%2F4.png)

- Ada-(La m√°s r√°pida)
$0.0004 -> $0.0016 / 1K tokens 
Es el m√°s r√°pido de los modelos enumerados, es una opci√≥n rentable para aplicaciones donde la velocidad es un factor cr√≠tico, como en aplicaciones de servicio al cliente o chatbot.

- Babbage
$0.0006 -> $0.0024 / 1K tokens
Es un poco m√°s lento que Ada, pero aun as√≠ ofrece una opci√≥n r√°pida y eficiente para tareas de procesamiento de lenguaje natural.

- Curie
$0.0030 -> $0.0120 / 1K tokens
Es m√°s caro que Ada y Babbage. Sin embargo, ofrece capacidades m√°s avanzadas que los modelos m√°s r√°pidos, lo que lo convierte en una buena opci√≥n para aplicaciones que requieren un procesamiento m√°s complejo.

- Davinci (el m√°s poderoso)
$0.0300 -> $0.120 / 1K tokens 
El modelo m√°s poderoso de la lista es Davinci, que ofrece las capacidades m√°s avanzadas para tareas de procesamiento de lenguaje natural. Sin embargo, es la opci√≥n m√°s cara de la lista. Es ideal para aplicaciones donde la precisi√≥n y las respuestas matizadas son fundamentales, como en escenarios complejos de atenci√≥n al cliente o proyectos de investigaci√≥n.

## 2.3 Configuraci√≥n de entorno local de OpenAI con Anaconda

Te guiar√© a trav√©s del proceso de configuraci√≥n de un entorno local para usar la API de OpenAI con Python. Empecemos con los requisitos previos.

### ‚úÖ Requisitos previos
Antes de comenzar, aseg√∫rate de tener los siguientes requisitos instalados en tu computadora:

1. Python versi√≥n 3.9 o superior: La API de OpenAI se integra estrechamente con Python, por lo que necesitar√°s tenerlo instalado en tu sistema operativo de preferencia. Puedes descargar la √∫ltima versi√≥n desde su sitio web oficial.

2. VSCode: En este editor de c√≥digo crearemos los scripts de Python para nuestra aplicaci√≥n. Puedes descargar la √∫ltima versi√≥n en el sitio oficial de VSCode.

3. Extensi√≥n de Python de VSCode: Para facilitar el uso de Python en el editor, instala la extensi√≥n desde su sitio oficial en el marketplace .

4. Anaconda: Es una herramienta popular para crear entornos de desarrollo para ciencia de datos y machine learning con Python. Aseg√∫rate de tener la √∫ltima versi√≥n de Anaconda instalada en tu sistema. Puedes descargarla desde el sitio web oficial de Anacondal.

### üêç Creaci√≥n de entorno virtual con Anaconda
Es una buena pr√°ctica utilizar entornos virtuales para aislar tus proyectos y dependencias. Sigue estos pasos para crear un entorno virtual con Anaconda:

1. Abre una terminal en tu computadora.

2. Ejecuta el siguiente comando para crear un nuevo entorno virtual llamado ‚ÄúNAME‚Äù (puedes reemplazar ‚ÄúNAME‚Äù con el nombre que desees, por ejemplo ‚Äúcurso_openai‚Äù):

```commandline
conda create -n NAME python==3.9
```

3. Una vez que se haya creado el entorno virtual, act√≠valo con el siguiente comando:

```commandline
conda activate NAME
```

4. Ahora est√°s trabajando dentro del entorno virtual ‚ÄúNAME‚Äù y puedes instalar las bibliotecas necesarias sin afectar tu instalaci√≥n principal de Python. Instala las librer√≠as necesarias con el siguiente comando*:

```commandline
conda install numpy pandas openai requests
```

> *Para instalar las librer√≠as en el entorno ‚ÄúNAME‚Äù deber√° estar activado con el paso anterior.

5. Cuando hayas terminado de trabajar en tu proyecto, puedes desactivar el entorno virtual con el siguiente comando:

```commandline
conda deactivate
```

Si quieres aprender a detalle a usar Anaconda y Jupyter Notebooks, puedes tomar el Curso de Entorno de Trabajo para Ciencia de Datos con Jupyter Notebooks y Anaconda üêç

### üîê Creaci√≥n de API Key
Antes de utilizar la API de OpenAI necesitar√°s una clave de API para acceder a los modelos. Sigue estos pasos para crear tu propia clave:

1. Visita la p√°gina de OpenAI para gestionar tus claves de API: https://platform.openai.com/account/api-keys.

2. Si a√∫n no tienes una cuenta de OpenAI, reg√≠strate y crea una nueva cuenta.

3. Una vez que hayas iniciado sesi√≥n en tu cuenta de OpenAI, ve a la secci√≥n de API Keys y crea una nueva clave.

4. Copia tu API Key e imp√≥rtala como variable de entorno de tu sistema operativo. Gu√°rdala como ‚ÄúOPENAI_API_KEY‚Äù de forma permanente. Esto ayudar√° a garantizar la seguridad de tu API Key al utilizarla de esta manera en tu c√≥digo. Puedes ver esta clase para configurarla en Ubuntu o macOS si no conoces el proceso.

5. Recuerda que para comenzar a utilizar la API, necesitar√°s tener al menos $5 d√≥lares de cr√©dito en tu tarjeta bancaria para utilizar los modelos y funcionalidades de la API.

### ‚ö†Ô∏èüíµ Costo del fine-tuning del modelo Davinci y alternativas
Es importante tener en cuenta que el fine-tuning del modelo Davinci de OpenAI puede tener un costo significativo. La tarifa de uso de Davinci se basa en el n√∫mero de tokens de entrada y salida utilizados durante el proceso de fine-tuning. Te recomiendo revisar la documentaci√≥n oficial de OpenAI para obtener informaci√≥n actualizada sobre los precios y las pol√≠ticas de uso en el siguiente enlace: https://openai.com/pricing

Para el proyecto de este curso el costo estimado por el fine-tuning ronda los $45 d√≥lares, utilizando un dataset con 1800 registros y el modelo Davinci.

Si deseas reducir el costo del fine-tuning, considera las siguientes alternativas:

1.  Reducir el tama√±o del dataset: Limitar la cantidad de registros utilizados en el proceso de fine-tuning puede ayudar a reducir los costos. Eval√∫a la posibilidad de seleccionar una muestra representativa de tus datos en lugar de utilizar todo el dataset.

2. Explorar modelos m√°s accesibles: Adem√°s del modelo Davinci, OpenAI ofrece modelos como Ada o Babbage, que son m√°s accesibles en t√©rminos de costo. Puedes explorar la opci√≥n de utilizar modelos alternativos seg√∫n tus necesidades y presupuesto.

## 2.4 Formato de datos para fine-tuning



## 2.5 Preparar datos para fine-tuning

## 2.6 Fine-tuning de modelo de OpenAI

## 2.7 ¬øC√≥mo usar PlayGround de OpenAI para probar modelos?

## 2.8 Pruebas al modelo con fine-tuning

## 2.9 Optimizar el modelo: ajuste de par√°metros en Playground

## 2.10 Validaci√≥n de modelos fine-tuned de OpenAI

## Quiz de fine-tuning de modelos de OpenAI

# 3 Integraci√≥n de modelo a aplicaci√≥n de chat

## 3.1 ¬øC√≥mo crear un chatbot con Telegram?

## 3.2 Procesando la entrada del usuario para el chatbot

## 3.3 Prueba de env√≠o de mensajes del chatbot

## 3.4 Funci√≥n main() del chatbot

## 3.5 Integraci√≥n del modelo de OpenAI a Telegram

## 3.6 Manejo de errores y excepciones de la API de OpenAI

## Quiz de integraci√≥n de LLM a chat

# 4 Conclusi√≥n

## 4.1 Recomendaciones finales y proyectos alternativos con el API de OpenAI